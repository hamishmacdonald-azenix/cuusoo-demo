# Automation Configuration

This folder contains all the YAML configuration files which are used to ingestion and transform data in the framework.

The core YAML config file is [master.yml](master.yml), which lists all of the top level "systems" that are to be executed in the framework. Systems are simply a physical grouping of "tasks" based on a business construct, and are generally aligned to business systems that data is being extracted from. 

The table below outlines the supported attributes for a given system.

## System Configuration

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **config_path** | The relative path under the current folder where the framework should search for all the "task" level configuration files | `ingestion/sql server/adventureworks` |
| **databricks_cluster_name** | The name of the Databricks cluster to use for the incremental load logic in ADF. The default cluster name will be deployed though the `databricks-config` process | `dp-dbr-clu-etl` |
| **enabled** | Boolean value indicating if the system is enabled or not | `true` or `false` |
| **pipeline_name** | The name of the pipeline to execute in ADF | `SQL_Import_Master` |
**system_name** | A unique, descriptive name for the system which will be passed to the ADF pipeline using a parameter named `system_name`, to assist in debugging and alerting processes, and to ensure that multiple instances of the same system are not executing concurrently | `SQL Server Import to Lake - AdventureWorks` |
| **system_type** | An identifier for the type of system being run - The value should always be the same for systems of the same type | `sql_import`

The framework currently supports importing the sources below:
- **SQL Server** - Currently only cloud-based SQL sources are supported using the out-of-the-box ADF hosted runtime and managed private endpoints, created through the Terraform deployment. You can [deploy your own Self-Hosted runtime](https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory) in Azure or on-premise and create a new ADF dataset to use the runtime. You can then modify the [SQL_Import_Parquet](data-factory/pipeline/SQL_Import_Parquet.json) ADF pipeline to include a conditional check to determine whether the existing [Generic_AzureSQL_MPE](data-factory/dataset/Generic_AzureSQL_MPE.json) dataset using the Azure hosted runtime or the new self-hosted runtime should be used
- **SharePoint** - Files hosted in a SharePoint document library can be imported. The framework uses an Azure Service Principal created through an App Registration with `Sites.Read.All` application permissions granted to the **Office 365 SharePoint Online** API listed under **APIs my organisation uses**. The `client id` and `client secret` (Generated by the app owner under **Certificates and secrets**) are they added to the deployed Key Vault instance manually and included in the task level configuration detailed below
- **CosmosDB** - Data can be imported from an existing CosmosDB service

The tables below outline the attribute configuration for each import type.

## Task Configuration

### SQL Server

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **source.incremental_column** | The name of the column to use for incremental loads. Only required when `load_type` is `incremental` | `ModifiedDate` |
| **source.incremental_column_data_type** | The data type for the incremental load column. Supplied values should be either `datetime` for date and date time data types, or any other applicable data type. Only required when `load_type` is `incremental` | `int` |
| **source.load_type** | The type of load being performed. For full loads, use `full` and for incremental loads use `incremental` | `full` |
| **source.object_name** | The name of the object and related folder where the source parquet files will be written to by ADF. Also used as part of the key identifier in the watermark table for incremental loads | `saleslt.customer` |
| **source.secrets.connection_string** | The name of the Key Vault secret deployed into the framework Key Vault instance which houses the connection string used for authentication. Note that only the `server` and `database` are required in the connection string if service principal authentication is being done, which is the preferred method | `sqlConnectionStringAdventureWorks` |
| **source.sql_query** | The query which returns the required result set from source. If you already have a **WHERE** clause, wrap the **SELECT** with the existing clause in a sub-query, as the incremental logic will append another **WHERE** clause onto the query automatically | `SELECT * FROM [SalesLT].[Customer]`
| **source.system_name** | This value is persisted in the incremental water mark delta table called `bronze.incremental_load_log`, used to store details for incremental loads from source | `adventureworkslt` |
| **start_date** | If this value is in the past, the task will be executed the first time the system runs through the master Azure function. Can be used to start a load at a particular time of day | `2022-05-11 00:00` |
| **target.file_path** | The logical path where the files will be written to in the deployed data lake from ADF. All files are written to an `unprocessed` directory and are then moved to a `processed` directory once they have been processed | `raw/adventureworks/unprocessed` |
| **target.file_type** | This value should be `parquet`. If other output file formats are required, those will need to be added in and the relevant ADF changes will need to be implemented to support this | `parquet` |
| **target.secrets.storage_account_name** | The name of the Key Vault secret which contains the name of the storage account which data is to be written to. The `storageAccountDataLake` secret is automatically deployed as part of the Terraform deployment, so no change is required  | `storageAccountDataLake` |
| **target.storage_container_name** | The name of the target storage container. Leave as the default value of `datalakestore` unless there is a valid reason to change it | `datalakestore`

### SharePoint

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **source.load_type** | The type of load to perform. If you only want to import files from source that have not already been imported into the target, specify `new_files`, otherwise use `all_files` | `new_files` |
| **source.secrets.azure_tenant_id** | The name of the Key Vault secret which stores the Azure tenant ID | `tenantId` |
| **source.secrets.client_id** | The name of the Key Vault secret which stores the service principal client ID that is used to authenticate to SharePoint | `sharepointAppClientID` |
| **source.secrets.client_secret** | The name of the Key Vault secret which stores the service principal client secret that is used to authenticate to SharePoint | `sharepointAppClientSecret` |
| **source.secrets.sharepoint_folder_path** | The name of the Key Vault secret which stores the folder path where files are to be read from in the given document library | `sharepointFolderPathLR10` |
| **source.sharepoint_host** | The name of the SharePoint host | `it.sharepoint.com` |
| **source.sharepoint_site_url** | The relative name of the SharePoint site where the document library is hosted | `sites/Reports` |
| **start_date** | If this value is in the past, the task will be executed the first time the system runs through the master Azure function. Can be used to start a load at a particular time of day | `2022-05-11 00:00` |
| **target.file_path** | The logical path where the files will be written to in the deployed data lake from ADF. All files are written to an `unprocessed` directory and are then moved to a `processed` directory once they have been processed | `raw/excel_reports/unprocessed` |
| **target.secrets.storage_account_name** | The name of the Key Vault secret which contains the name of the storage account which data is to be written to. The `storageAccountDataLake` secret is automatically deployed as part of the Terraform deployment, so no change is required  | `storageAccountDataLake` |
| **target.storage_container_name** | The name of the target storage container. Leave as the default value of `datalakestore` unless there is a valid reason to change it | `datalakestore`

### CosmosDB

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **source.collection_name** | The name of the CosmosDB collection to be used as a source | `events` |
| **source.database_name** | The name of the source CosmosDB database | `demo` |
| **source.incremental_column** | The name of the column to use for incremental loads. Only required when `load_type` is `incremental` | `ModifiedDate` |
| **source.incremental_column_data_type** | The data type for the incremental load column. Supplied values should be either `datetime` for date and date time data types, or any other applicable data type. Only required when `load_type` is `incremental` | `datetime` |
| **source.load_type** | The type of load being performed. For full loads, use `full` and for incremental loads use `incremental` | `incremental` |
| **source.object_name** | The name of the object and related folder where the source parquet files will be written to by ADF. Also used as part of the key identifier in the watermark table for incremental loads | `AssessmentMoveCompleted` |
| **source.secrets.account_endoint** | The name of the Key Vault secret which stores the CosmosDB account endpoint | `cosmosAccountEndpoint` |
| **source.secrets.account_key** | The name of the Key Vault secret which stores the CosmosDB account key | `cosmosAccountKey` |
| **source.sql_query** | The CosmosDB SQL API query which returns the required result set. If you already have a **WHERE** clause, wrap the **SELECT** with the existing clause in a sub-query, as the incremental logic will append another **WHERE** clause onto the query automatically | `SELECT * FROM c WHERE c.EventTypeName = 'AssessmentMoveCompleted'`
| **source.system_name** | This value is persisted in the incremental water mark delta table called `bronze.incremental_load_log`, used to store details for incremental loads from source | `Hub` |
| **translator** | This is the ADF tabular translator required to map the source attributes from CosmosDB to the sink columns | `type: TabularTranslator ...`
| **start_date** | If this value is in the past, the task will be executed the first time the system runs through the master Azure function. Can be used to start a load at a particular time of day | `2022-05-11 00:00` |
| **target.file_path** | The logical path where the files will be written to in the deployed data lake from ADF. All files are written to an `unprocessed` directory and are then moved to a `processed` directory once they have been processed | `raw/hub/unprocessed` |
| **target.file_type** | This value should be `parquet`. If other output file formats are required, those will need to be added in and the relevant ADF changes will need to be implemented to support this | `parquet` |
| **target.secrets.storage_account_name** | The name of the Key Vault secret which contains the name of the storage account which data is to be written to. The `storageAccountDataLake` secret is automatically deployed as part of the Terraform deployment, so no change is required  | `storageAccountDataLake` |
| **target.storage_container_name** | The name of the target storage container. Leave as the default value of `datalakestore` unless there is a valid reason to change it | `datalakestore`

### Raw to Bronze loads
In addition to the sources listed above, the framework supports the execution of a [generic Databricks notebook](databricks/notebooks/ingestion/Load_Raw_To_Bronze.py) to read the files delivered to the `unprocessed` directory in the raw area of the lake to a delta table in the bronze area of the lake. The configuration files are generally kept in a source-aligned folder structure under a `raw to bronze` top level folder.

The table below outlines the attribute configuration for a `raw` to `bronze` data load.

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **dependencies** | A list of dependant tasks which need to succeed before the current task will be executed | `- ingestion/sql server/adventureworks/saleslt_sales_order_detail.yml`
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **source.file_format** | The file type which needs to be ingested. Supported formats are `parquet`, `sas7dbdat`, `csv`, `json` and `Excel` | `parquet` |
| **source.file_name_search** | The search term used to locate files that match a certain naming convention. The search term must be a file prefix | `saleslt.salesorderdetail_` |
| **source.file_path** | The logical path in the data lake which should be searched for the input file/s | `raw/adventureworks/unprocessed` |
| **source.secrets.storage_account_name** | The name of the Key Vault secret which contains the name of the storage account where data is to be retrieved from | `storageAccountDataLake` |
| **source.storage_container_name** | The name of the source storage container. Leave as the default value of `datalakestore` unless there is a valid reason to change it | `datalakestore`
| **start_date** | If this value is in the past, the task will be executed the first time the system runs through the master Azure function. Can be used to start a load at a particular time of day | `2022-05-11 00:00` |
| **target.delta_table_name** | The name of the delta table that the source data should be written to | `bronze.adv_saleslt_sales_order_detail` |
| **target.key_column_list** | Required if the load type is `incremental`; a comma separted list of key columns in the target table - A generic MERGE will be executed to load incremental changes into the target table based on this column list | `SalesOrderDetailID` |
| **target.load_type** | If an incremental load is required then `incremental`, otherwise `full` | `incremental` |

### Transformation
The framework has support for synchronously executing Databricks jobs. The related tasks are generally kept in a set of files under a `transformation` top level folder

The table below outlines the attribute configuration for a transformation task.

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **dependencies** | A list of dependant tasks which need to succeed before the current task will be executed | `- ingestion/sql server/adventureworks/saleslt_sales_order_detail.yml`
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **job_list** | A list of Databricks jobs which should be executed. The items listed here should match the names of existing Databricks jobs in the deployed Databricks instance for the relevant environment | `- Create-Tables ...` |
| **run_type** | The way in which the jobs should be executed. If the jobs need to be executed sequentially, then use `sequential` | `sequential` |
| **start_date** | If this value is in the past, the task will be executed the first time the system runs through the master Azure function. Can be used to start a load at a particular time of day | `2022-05-11 00:00` |

### Delta Table Maintenance
The framework has support for executing optimisation commands to improve the performance of delta table on Databricks.

The table below outlines the attribute configuration for a delta table maintenance task.

| Attribute Name | Description | Example |
| -------------- | ----------- | ------- |
| **cron_schedule** | A CRON schedule which determines when a task should be executed. If a task fails, it will automatically be required the next time the [master Azure function](functions/master-schedule-run/__init__.py) runs, which is every 5 minutes by default. The frequency can be adjusted in the [function.json](functions/master-schedule-run/function.json) file | `0 */12 * * *` |
| **enabled** | Boolean value indicating if the task is enabled or not | `true` or `false` |
| **tables** | A list of tables which need to be optimised. Attributes include the items listed below: <br><br><li>**compute_statistics_all_columns** - A `true` or `false` to indicate if all the columns should have statistics recomputed<br><li>**table_name** - The name of the table to be optimised<br><li>**table_properties** - A list of table properties which need to be set for the table<br><li>**vacuum_retention_hours** - Only required if a table is to be vacuumed, the number of hours to retain history for<br><li>**z_order_by** - Only required if Z Ordering is to be applied to a table, a comma separated list of columns to Z order by | `- compute_statistics_all_columns: false`|

## Deployment

In order to synchoronise all the YAML configuration files from the repo to the Azure storage account, the [azcopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) utility is utilised to perform a directory synchronisation. This can be performed by using the [azure-file-sync.sh](azure-file-sync.sh) Bash script locally or in your DevOps pipeline. azcopy authentication to the Azure storage account can be optained by setting the environment varible `AZCOPY_SPA_CLIENT_SECRET` to the client secret of the DevOps service principal used for deployments into the target Azure tenancy.